{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyfXxQyNRwfcHDfZOaYs/S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tvHCuywRjQF","executionInfo":{"status":"ok","timestamp":1747320101944,"user_tz":240,"elapsed":696,"user":{"displayName":"Ravi Charan","userId":"03045932878504377108"}},"outputId":"1ebad87a-aeae-4930-f5c9-c92abba8e48f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","words = open('/content/drive/MyDrive/Colab Notebooks/names.txt', 'r').read().splitlines()\n","\n","import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","source":["chars = sorted(list(set(''.join(words))))\n","stoi = {s:i+1 for i,s in enumerate(chars)}\n","stoi['.'] = 0\n","itos = {i:s for s,i in stoi.items()}\n","vocab_size = len(itos)\n","block_size = 3"],"metadata":{"id":"lHVXbRh3ZBqK","executionInfo":{"status":"ok","timestamp":1747320101955,"user_tz":240,"elapsed":8,"user":{"displayName":"Ravi Charan","userId":"03045932878504377108"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#build dataset\n","block_size = 3\n","def build_dataset(words):\n","  X,Y = [],[]\n","\n","  for w in words:\n","    context = [0]*block_size\n","    for ch in w + '.':\n","      ix = stoi[ch]\n","      X.append(context)\n","      Y.append(ix)\n","      #print(''.join(itos[i] for i in context), '---->', itos[ix])\n","      context = context[1:] + [ix]\n","\n","  X = torch.tensor(X)\n","  Y = torch.tensor(Y)\n","  print(X.shape, Y.shape)\n","  return X, Y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","n1 = int(0.8*len(words))\n","n2 = int(0.9*len(words))\n","\n","Xtr, Ytr = build_dataset(words[:n1])\n","Xdev, Ydev = build_dataset(words[n1:n2])\n","Xte, Yte = build_dataset(words[n2:])"],"metadata":{"id":"MUSCmJRjiYlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Linear:\n","\n","  def __init__(self, fan_in, fan_out, bias=True):\n","    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n","    self.bias = torch.zeros(fan_out) if bias else None\n","\n","  def __call__(self, x):\n","    self.out = x @ self.weight\n","    if self.bias is not None:\n","      self.out += self.bias\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.weight] + ([] if self.bias is None else [self.bias])\n","\n","class BatchNorm1d:\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.momentum = momentum\n","    self.training = True\n","\n","    #parameters (trained with backprop)\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","    #buffers (trained with a running 'momentum update')\n","    self.running_mean =  torch.zeros(dim)\n","    self.running_var = torch.ones(dim)\n","\n","  def __call__(self, x):\n","    #calculate forward pass\n","    if self.training:\n","      xmean = x.mean(0, keepdim=True)\n","      xvar = xvar(0, keepdim=True)\n","    else:\n","      xmean = self.running_mean\n","      xvar = self.running_var\n","    xhat = (x-xmean) / (torch.sqrt(xvar + self.eps))\n","    self.out = self.gamma * xhat + self.beta\n","\n","    #update the buffers\n","    if self.training:\n","      with torch.no_grad():\n","        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n","        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n","\n","class Tanh:\n","  def __call__(self, x):\n","    self.out = torch.tanh(x)\n","    return self.out\n","  def parameters(self):\n","    return []\n","\n","n_embd = 10\n","n_hidden = 100\n","g = torch.Generator().manual_seed(2147483647)\n","\n","C = torch.randn((vocab_size, n_embd),     generator=g)\n","layers = [\n","    Linear(n_embd * block_size, n_hidden), Tanh(),\n","    Linear(n_hidden, n_hidden), Tanh(),\n","    Linear(n_hidden, n_hidden), Tanh(),\n","    Linear(n_hidden, n_hidden), Tanh(),\n","    Linear(n_hidden, n_hidden), Tanh(),\n","    Linear(n_hidden, vocab_size),\n","]\n","\n","with torch.no_grad():\n","  #last layer make less confident\n","  layers[-1].weight *= 0.1\n","  #all other layers apply gain\n","  for layer in layers[:-1]:\n","    if isinstance(layer, Linear):\n","      layer.weight *= 5/3\n","\n","parameters = [C] + [p for layer in layers for p in layer.parameters()]\n","print(sum(p.nelement() for p in parameters))\n","for p in parameters:\n","  p.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dt61cHM8RoMx","executionInfo":{"status":"ok","timestamp":1747320101975,"user_tz":240,"elapsed":11,"user":{"displayName":"Ravi Charan","userId":"03045932878504377108"}},"outputId":"355568da-0da5-41ab-c863-72413a44942d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["46497\n"]}]},{"cell_type":"code","source":["max_steps = 200000\n","batch_size = 32\n","lossi = []\n","ud = []\n","\n","for i in range(max_steps):\n","\n","  #minbatch construct\n","  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","  Xb, Yb = Xtr[ix], Ytr[ix]\n","\n","  #forward pass\n","  emb =C[Xb]\n","  x = emb.view(emb.shape[0], -1)\n","  for layer in layers:\n","    x = layer(x)\n","  loss = F.cross_entropy(x, Yb)\n","\n","  #backward pass\n","  for layer in layers:\n","    layer.out.retain_grad()\n","  for p in parameters:\n","    p.grad = None\n","  loss.backward()\n","\n","  #update\n","  lr = 0.1 if i < 100000 else 0.01\n","  for p in parameters:\n","    p.data += -lr * p.grad\n","\n","  #track stats\n","  if i % 10000 == 0:\n","    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","  lossi.append(loss.log10().item())\n","  with torch.no_grad():\n","    ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in parameters])\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"zIluv3jHYnMZ","executionInfo":{"status":"error","timestamp":1747320834973,"user_tz":240,"elapsed":12,"user":{"displayName":"Ravi Charan","userId":"03045932878504377108"}},"outputId":"0521bc50-b0bd-4516-a333-c719e5572a2c"},"execution_count":13,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"incomplete input (<ipython-input-13-ddb180b1e407>, line 27)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-ddb180b1e407>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    for p in parameters:\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"]}]},{"cell_type":"code","source":["#visualize histograms\n","plt.figure(figsize=(20, 4))\n","legends = []\n","for i,layer in enumerate(layer[:-1]):\n","  if isinstance(layer, Tanh):\n","    t = layer.out\n","    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % if (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs > 0.97).float().mean()*100))\n","    hy, hx = torch.histogram(t, density=True)\n","    plt.plot(hx[:-1].detach(), hy.detach())\n","    legends.append(f'layer {i} ({layer.__class__.__name__})')\n","plt.legends(legends);\n","plt.title('activation distribution')"],"metadata":{"id":"hBIju6a2cAsf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualize histograms\n","plt.figure(figsize=(20, 4))\n","legends = []\n","for i,layer in enumerate(layer[:-1]):\n","  if isinstance(layer, Tanh):\n","    t = layer.out.grad\n","    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % if (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs > 0.97).float().mean()*100))\n","    hy, hx = torch.histogram(t, density=True)\n","    plt.plot(hx[:-1].detach(), hy.detach())\n","    legends.append(f'layer {i} ({layer.__class__.__name__})')\n","plt.legends(legends);\n","plt.title('gradient distribution')"],"metadata":{"id":"Jycq4NR1di4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize histograms\n","plt.figure(figsize=(20, 4)) # width and height of the plot\n","legends = []\n","for i,p in enumerate(parameters):\n","  t = p.grad\n","  if p.ndim == 2:\n","    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n","    hy, hx = torch.histogram(t, density=True)\n","    plt.plot(hx[:-1].detach(), hy.detach())\n","    legends.append(f'{i} {tuple(p.shape)}')\n","plt.legend(legends)\n","plt.title('weights gradient distribution');"],"metadata":{"id":"fdYOkvGl_6Gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20, 4))\n","legends = []\n","for i,p in enumerate(parameters):\n","  if p.ndim == 2:\n","    plt.plot([ud[j][i] for j in range(len(ud))])\n","    legends.append('param %d' % i)\n","plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n","plt.legend(legends);"],"metadata":{"id":"-MspqV8z_7U8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad() # this decorator disables gradient tracking\n","def split_loss(split):\n","  x,y = {\n","      'train': (Xtr, Ytr),\n","      'val': (Xdev, Ydev),\n","      'test': (Xte, Yte),\n","  }[split]\n","  emb = C[x]\n","  embcat = emb.view(emb.shape[0], -1)\n","  hpreact = torch.tanh(embcat @ W1 + b1)\n","  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim = True)\n","  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n","  logits = hpreact @ W2 + b2\n","  loss = F.cross_entropy(logits, y)\n","  print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')"],"metadata":{"id":"IQmwzJwpeScQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["g = torch.Generator().manual_seed(2147483647 + 10)\n","\n","for _ in range(20):\n","\n","  out = []\n","  context = [0] * block_size\n","  while True:\n","    emb = C[torch.tensor([context])]\n","    h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n","    logits = h @ W2 + b2\n","    probs = F.softmax(logits, dim = 1)\n","    ix = torch.multinomial(probs, num_samples = 1, generator=g).item()\n","    context = context[1:]+[ix]\n","    out.append(ix)\n","    if ix == 0:\n","      break\n","  print(''.join(itos[i] for i in out))"],"metadata":{"id":"lRBDOm2peSQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oOJVXnyWeSNN"},"execution_count":null,"outputs":[]}]}